# Разработка полносвязной нейронной сети на базе фреймворка TensorFlow Python для классификации электронных писем

### Работу выполнили: студенты группы 15.27Д-ПИ02/25б Галкина Мария и Меджидова Элина

#### ВВЕДЕНИЕ
##### Актуальность работы:

Проблема спама в электронной почте остаётся одной из наиболее острых в современном цифровом мире. По данным 2025 года, почтовые серверы обрабатывают десятки миллиардов писем ежедневно, и от 18 до 44% всего почтового трафика составляет спам и фишинговые письма. Только на Яндекс 360 за неполный 2025 год было заблокировано 9,8 миллиарда спам-писем, а на Почте Mail — более 13,7 миллиардов спам-сообщений в первой половине года.

Традиционные методы фильтрации, основанные на простых правилах, сегодня уже не справляются: спамеры используют маскировку, персонализацию и методы социальной инженерии. Следовательно, необходимы «интеллектуальные» системы, способные обучаться и адаптироваться. Именно такие системы создаются на основе машинного обучения и нейронных сетей.

Таким образом, актуальность работы определяется несколькими ключевыми факторами:

-Острая общественная потребность: спам причиняет ущерб миллионам пользователей ежедневно.

-Научная обоснованность: нейронные сети доказали свою эффективность в решении задачи классификации текстов.


Разработка полносвязной нейронной сети на базе TensorFlow для классификации электронной почты представляет собой актуальный, теоретически обоснованный и практически значимый проект, который развивает навыки работы с современными технологиями искусственного интеллекта.

#### ГЛАВА I
##### Аналоги нейронной сети:
При анализе существующих аналогов в области автоматической классификации электронных писем можно выделить пять наиболее успешных примеров, демонстрирующих различные подходы к решению данной задачи.

1. Gmail от компании Google.
   Gmail обрабатывает миллиарды писем ежедневно и блокирует около 100 млн спам-писем в день. Система использует несколько инновационных подходов:

TensorFlow-based Deep Learning: Google применяет фреймворк TensorFlow для построения своей нейросетевой архитектуры; машинное обучение с обратной связью: система анализирует сигналы от пользователей (отметка как спам), историю отправителя, репутацию IP-адреса и доменов; Gemini AI интеграция: современная версия использует продвинутые модели AI для анализа вовлечённости, технических деталей и соответствия стандартам; оптическое распознавание символов (OCR): Для защиты от спама на основе изображений.

2. Microsoft Defender for Office 365.
   Встроённое в Outlook и Microsoft 365 решение использует:

Контент-фильтры: сканирование заголовков и тела писем на наличие спам-контента; Blocklists: блокировка известных спаммеров через DNS-проверки; AI-driven фильтрация: динамический анализ с использованием машинного обучения; Spam Confidence Level (SCL) скоринг: каждому письму присваивается скор (0-9), определяющий его судьбу; новая функция Mail Bombing Detection (2025): отслеживает аномальные всплески объёма писем, используя исторические паттерны и анализ содержимого; интеграция с Threat Intelligence: Real-time защита от фишинга, вредоноса и целевых атак.

3. SpamAssassin.
   Одно из самых популярных открытых решений (изначально создано в 2001 году):

Bayesian Filtering: основан на вероятностных методах, схожих с классическими подходами ML; модульная архитектура: содержит множество встроенных правил и тестов; Plain-text конфигурация: легко добавлять новые правила; автообучение; требования к обучению: Минимум 1,000 спам-писем и 1,000 легитимных писем для эффективной работы.

4. Mimecast Targeted Threat Protection.

​Layered Spam Scanning: комбинация собственных технологий с третьими сторонами; URL Protection: Real-time сканирование всех ссылок в письмах; Impersonation Protection: защита от социальной инженерии через анализ аномалий в заголовках и доменах.

5. Proofpoint Email Protection.
   Корпоративное решение, защищающее тысячи организаций:

Анализ 200,000+ атрибутов сообщений: экспертный уровень детализации; Machine Learning с NexusAI: адаптивное обучение на новых типах атак; Reputation-based фильтры: отслеживание репутации отправителей в режиме реального времени; Quarantine & Digest система: пользователи получают ежедневные дайджесты подозрительных писем.

#### ГЛАВА II
##### Архитектура нейронной сети

Модель представляет собой полносвязную нейронную сеть (MLP - Multi-Layer Perceptron) с 7 слоями (3 Dense + 2 Activation + 2 Dropout), из них 2 скрытых слоя, построенную на базе TensorFlow/Keras 2.19.0. Архитектура Sequential содержит 164 параметра: входной слой принимает 5 признаков, первый скрытый слой - 12 нейронов, второй - 6 нейронов, выходной слой - 2 нейрона для бинарной классификации с softmax.

Общее число параметров: 164 (все обучаемые)
Глубина: 3 Dense слоя
Ширина: 12 → 6 → 2 (funnel shape)
Тип: Binary classification MLP
Framework: TensorFlow/Keras 2.19.0

Вход (5 признаков) 
  ↓
Dense(12, HeUniform, L2=0.001) + ReLU + Dropout(0.2)  ← Скрытый слой 1
  ↓
Dense(6, HeUniform, L2=0.001) + ReLU + Dropout(0.2)   ← Скрытый слой 2  
  ↓
Dense(2, L2=0.001) + Softmax               ← Выходной слой

##### Подсчет параметров:
Dense(12): 5×12 + 12 = 72
Dense(6): 12×6 + 6 = 78
Dense(2): 6×2 + 2 = 14
Итого: 164

Модель построена как простая полносвязная нейронная сеть с тремя слоями, потому что у нас всего пять признаков и двадцать тысяч примеров — это небольшой датасет, где сложная архитектура просто переобучится. Сначала входные данные расширяются до двенадцати нейронов, чтобы сеть могла найти комбинации признаков, например, связать много ссылок с низким рейтингом отправителя. Потом сжимается до шести нейронов, чтобы отобрать самые важные паттерны. Наконец, два нейрона на выходе дают вероятности двух классов.

ReLU выбрана в скрытых слоях, потому что она работает быстро — в положительной области градиент равен единице, без затухания, как в сигмоиде. Это особенно важно для маленькой сети, чтобы обучение не застревало. Dropout на уровне двадцать процентов после каждого ReLU случайно выключает часть нейронов во время тренировки, заставляя сеть не полагаться на отдельные "звездные" нейроны и работать как ансамбль. Инициализация HeUniform идеально подходит под ReLU, распределяя веса так, чтобы сигнал не затухал и не взрывался на первых слоях.

L2-регуляризация 0.001 на всех слоях добавляет небольшой штраф за большие веса в функцию потерь, что предотвращает переобучение. Adam с learning rate 0.0005 дает стабильное снижение потерь без осцилляций, а батч 128 обеспечивает хороший баланс между скоростью и качеством градиентов — меньше будет шумно, больше сеть может застрять в локальном минимуме. ReduceLROnPlateau автоматически уменьшает скорость обучения, когда валидационная потеря перестает падать, позволяя тонко настроить модель без ручного вмешательства.

Softmax на выходе нужен, потому что функция потерь sparse_categorical_crossentropy ожидает вероятности, суммирующиеся в единицу для двух классов. Это лучше, чем одиночный sigmoid, так как sparse_categorical_crossentropy требует на выходе столько нейронов, сколько классов — два для спама и не спама. Softmax превращает два числа в вероятности, которые всегда суммируются в единицу. Sigmoid дал бы одну вероятность спама от нуля до единицы, но без гарантии калибровки — могла бы выйти сто десять процентов или минус пять. Плюс softmax естественнее обучает сеть понимать взаимоисключающие классы через нормализованные логитты.

Такая архитектура оптимальна именно для табличных данных спам-классификации: простая, быстрая в обучении и обобщает хорошо, достигая девяносто пяти процентов точности без переобучения.

##### Описание параметров и гиперпараметров нейронной сети

##### Параметры слоёв
Слой 1 - Dense(12): Вход: 5 → Выход: 12. Параметры: 5×12 весов + 12 bias = 72. Инициализация: he_uniform — Uniform(-√(6/5), √(6/5)) = [-0.89, 0.89]. Регуляризация: L2=0.001 (штраф 0.001×∑w² всех весов)

Слой 2 - ReLU: f(x) = max(0, x) — 25-30% нейронов становятся нулем

Слой 3 - Dropout(0.2): 20% нейронов → 0 во время обучения, scale ×1.25 при инференсе

Слой 4 - Dense(6): Вход: 12 → Выход: 6. Параметры: 12×6 весов + 6 bias = 78. Инициализация: he_uniform — Uniform(-√(6/12), √(6/12)) = [-0.63, 0.63]

Слой 5 - ReLU + Слой 6 - Dropout(0.2): Аналогично слоям 2-3

Слой 7 - Dense(2): Вход: 6 → Выход: 2 (классы). Параметры: 6×2 весов + 2 bias = 14. Инициализация: по умолчанию (glorot_uniform)

Итого параметров: 72 + 78 + 14 = 164

Выходной слой - Softmax: P_i = exp(z_i)/∑exp(z_j)

##### Гиперпараметры обучения

Разделение данных:

Обучающая: 12,000 (60%)
Валидационная: 4,000 (20%) 
Тестовая: 4,000 (20%)
Stratify=Y сохраняет пропорции 91:9

ReLU выбрана в скрытых слоях, потому что она работает быстро — в положительной области градиент равен единице, без затухания, как в сигмоиде. Это особенно важно для маленькой сети, чтобы обучение не застревало. Dropout на уровне двадцать процентов после каждого ReLU случайно выключает часть нейронов во время тренировки, заставляя сеть не полагаться на отдельные "звездные" нейроны и работать как ансамбль. Инициализация HeUniform идеально подходит под ReLU, распределяя веса так, чтобы сигнал не затухал и не взрывался на первых слоях.

L2-регуляризация 0.001 на всех слоях добавляет небольшой штраф за большие веса в функцию потерь, что предотвращает переобучение. Adam с learning rate 0.0005 дает стабильное снижение потерь без осцилляций, а батч 128 обеспечивает хороший баланс между скоростью и качеством градиентов — меньше будет шумно, больше сеть может застрять в локальном минимуме. ReduceLROnPlateau автоматически уменьшает скорость обучения, когда валидационная потеря перестает падать, позволяя тонко настроить модель без ручного вмешательства.

Softmax на выходе нужен, потому что функция потерь sparse_categorical_crossentropy ожидает вероятности, суммирующиеся в единицу для двух классов. Это лучше, чем одиночный sigmoid, так как sparse_categorical_crossentropy требует на выходе столько нейронов, сколько классов — два для спама и не спама. Softmax превращает два числа в вероятности, которые всегда суммируются в единицу. Sigmoid дал бы одну вероятность спама от нуля до единицы, но без гарантии калибровки — могла бы выйти сто десять процентов или минус пять. Плюс softmax естественнее обучает сеть понимать взаимоисключающие классы через нормализованные логитты.

##### Метрики НС
Test Accuracy: 95.32%
Test Loss: 13.63%

##### Описание DataSet
Для обучения и тестирования разработанной полносвязной нейронной сети был использован специальный DataSet объёмом 20000 строк для задачи классификации электронных писем по категориям "Спам"/"Не спам" с сайта Kaggle.

Ссылка на DataSet: https://www.kaggle.com/datasets/smayanj/spam-detection-dataset.

Датасет содержит пять входных признаков, представляющих различные характеристики электронного письма:

1) num_links: Количество гиперссылок в тексте письма (Числовой). Большое количество ссылок часто коррелирует со спам-рассылками.
2) num_words: Общее количество слов в содержимом письма (Числовой). Спам-сообщения могут быть как краткими, так и неестественно объемными.
3) has_offer: Наличие коммерческого предложения (двоичная переменная) (Бинарный).
4) sender_score: Бинарный признак, показывающий репутационную оценку отправителя в системе.
5) all_caps: Бинарный признак (0 или 1), фиксирующий, написано ли письмо полностью заглавными буквами (частый маркер спам‑сообщений).

Целевая переменная представляет собой бинарную классификацию с двумя классами:

0 — письмо не относится к категории спама (legitimate email).
1 — письмо классифицируется как спам (spam).

Данный DataSet позволяет эффективно обучать нейронную сеть распознавать паттерны, характерные для спам‑сообщений, на основе количественных и бинарных признаков текста и метаданных отправителя.

##### Алгоритмы, реализованные в нейронной сети
В ходе реализации проекта по созданию нейронной сети для бинарной классификации электронных писем (спам / не спам) на базе фреймворка TensorFlow был применен ряд ключевых алгоритмов и методов машинного обучения, обеспечивающих эффективное обучение и высокую обобщающую способность модели. Ниже приведено подробное описание ключевых алгоритмов, задействованных в архитектуре модели.

1) Алгоритм оптимизации Adam (градиентный спуск). Adam вычисляет индивидуальные адаптивные оценки скорости обучения для каждого параметра нейронной сети, используя оценки первого (среднее) и второго (дисперсия) моментов градиентов. Это позволяет модели быстро сходиться к минимуму функции потерь даже на данных с большим количеством шума. В коде используется скорость обучения (learning rate) 0.0005.
2) Функция активации ReLU (Rectified Linear Unit). В скрытых слоях нейронной сети используется функция активации ReLU. Она необходима для внесения нелинейности в модель, что позволяет сети находить сложные зависимости между признаками спама. Применена в качестве функции активации для всех нейронов скрытых слоев (Activation('relu')).
3) Функция активации выходного слоя SoftMax. Так как в выходном слое модели используются 2 нейрона (соответствующие классам «Не спам» и «Спам»), для получения итогового предсказания применяется функция SoftMax. Она преобразует выходы нейронов (логиты) в распределение вероятностей, сумма которых равна 1. Использована в выходном слое сети (Activation('softmax')).
4) Стандартизация признаков (StandardScaler). На этапе подготовки данных (блок 3 в программном коде) применяется алгоритм стандартизации. Входные признаки писем (количество ссылок, количество слов, рейтинг отправителя и др.) имеют разные диапазоны значений, что может негативно сказаться на скорости сходимости нейронной сети. Для каждого числового признака вычисляется его среднее значение (mean) и стандартное отклонение (std) на обучающей выборке. Затем проводится преобразование: X_scaled = (X - mean) / std. В результате распределение каждого признака приводится к нулевому среднему и единичной дисперсии, что ускоряет сходимость градиентного спуска. Выполнена с помощью StandardScaler() из библиотеки sklearn.preprocessing.
5) L2‑регуляризация. Для предотвращения переобучения модели (ситуации, когда сеть просто «запоминает» тренировочные примеры, но плохо работает на новых письмах) в слоях Dense применена L2-регуляризация с коэффициентом lambda = 0.001. Алгоритм добавляет штрафное слагаемое к функции потерь, пропорциональное сумме квадратов всех весов модели. Добавлена к каждому полносвязному слою через параметр kernel_regularizer=tf.keras.regularizers.l2(l2_reg).

Разработанная архитектура демонстрирует синергию алгоритмов:
Стандартизация подготавливает данные для эффективного градиентного спуска.
ReLU обеспечивает нелинейность и скорость вычислений в скрытых слоях.
Softmax интерпретирует выходные данные как вероятности.
Adam оптимизирует веса с адаптивной скоростью обучения.
L2‑регуляризация контролирует сложность модели, предотвращая переобучение.

Такая комбинация позволяет достичь высокой точности классификации при сохранении обобщающей способности.

#### ЗАКЛЮЧЕНИЕ
##### Перспективы развития:
Наш проект — это работающий прототип, который имеет четкие пути развития в трех ключевых направлениях:

1. Улучшение точности и удобства модели.
Расширение и обогащение датасета: сбор собственной коллекции писем (с согласия пользователей) для обучения на более актуальных и релевантных примерах, что повысит точность распознавания современного спама.
Внедрение современных архитектур: переход от полносвязной сети к более мощным моделям, например, LSTM или Transformer (BERT), которые учитывают контекст и последовательность слов, что критично для понимания смысла письма.
Создание пользовательского интерфейса: разработка простого веб-интерфейса  или десктоп-приложения, где любой пользователь сможет вставить текст письма и мгновенно получить результат классификации.
2. Добавление функций обучения на ходу (online learning): Внедрение механизма, при котором пользователь, помечая письмо как спам/не спам, будет дообучать модель, делая ее персонализированной и адаптивной.
3. Внедрение в реальную среду.
