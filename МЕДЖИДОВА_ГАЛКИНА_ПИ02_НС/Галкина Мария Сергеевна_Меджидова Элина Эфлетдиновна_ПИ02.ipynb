import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# Фиксация случайности для воспроизводимости
np.random.seed(42)
tf.random.set_seed(42)

print("TensorFlow version:", tf.__version__)
print("=" * 60)

# 1. ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ
print("1. Загрузка данных...")
dataset = pd.read_csv('dataset_prob.csv')

# Определяем признаки и целевую переменную
feature_columns = ['num_links', 'num_words', 'has_offer', 'sender_score', 'all_caps']
target_column = 'is_spam'

X = dataset[feature_columns].apply(pd.to_numeric, errors='coerce').fillna(0).values
Y = dataset[target_column].map({0: 0, 1: 1}).fillna(0).values

print(f"Размер датасета: {X.shape[0]} образцов, {X.shape[1]} признаков")

# 2. РАЗДЕЛЕНИЕ НА ОБУЧЕНИЕ/ВАЛИДАЦИЮ/ТЕСТ (60%/20%/20%)
print("\n2. Разделение данных...")
X_train_full, X_test, Y_train_full, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=42, stratify=Y
)
X_train, X_val, Y_train, Y_val = train_test_split(
    X_train_full, Y_train_full, test_size=0.25, random_state=42, stratify=Y_train_full
)

# 3. НОРМАЛИЗАЦИЯ ПРИЗНАКОВ
print("\n3. Нормализация...")
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

print(f"Обучающая выборка: {X_train.shape[0]} образцов")
print(f"Валидационная выборка: {X_val.shape[0]} образцов")
print(f"Тестовая выборка: {X_test.shape[0]} образцов")

# 4. ПАРАМЕТРИЗОВАННАЯ МОДЕЛЬ С ReLU
def create_spam_model(input_dim=5, hidden_units=[12, 6], dropout_rate=0.2, l2_reg=0.001):
    """
    Создает нейронную сеть для классификации спама

    Args:
        input_dim: количество входных признаков
        hidden_units: список размеров скрытых слоев
        dropout_rate: вероятность dropout
        l2_reg: коэффициент L2-регуляризации
    """
    model = Sequential()

    # Первый скрытый слой
    model.add(Dense(
        hidden_units[0],
        input_shape=(input_dim,),
        kernel_initializer='he_uniform',
        kernel_regularizer=tf.keras.regularizers.l2(l2_reg)
    ))
    model.add(Activation('relu'))
    model.add(Dropout(dropout_rate))

    # Дополнительные скрытые слои
    for units in hidden_units[1:]:
        model.add(Dense(
            units,
            kernel_initializer='he_uniform',
            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)
        ))
        model.add(Activation('relu'))
        model.add(Dropout(dropout_rate))

    # Выходной слой
    model.add(Dense(2, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))
    model.add(Activation('softmax'))

    return model

# Создание и просмотр модели
print("\n4. Создание модели...")
model = create_spam_model(
    input_dim=X.shape[1],
    hidden_units=[12, 6],
    dropout_rate=0.2,
    l2_reg=0.001
)

model.summary()

# 5. КОМПИЛЯЦИЯ МОДЕЛИ
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=Adam(learning_rate=0.0005),
    metrics=['accuracy']
)

# 6. CALLBACK'и ДЛЯ КОНТРОЛЯ ОБУЧЕНИЯ (только ReduceLROnPlateau)
callbacks = [
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1
    )
]

# 7. ОБУЧЕНИЕ МОДЕЛИ
print("\n5. Обучение модели...")
history = model.fit(
    X_train, Y_train,
    batch_size=128,
    epochs=100,
    verbose=1,
    validation_data=(X_val, Y_val),
    callbacks=callbacks
)

# 8. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ ОБУЧЕНИЯ
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# График потерь
ax1.plot(history.history['loss'], 'g-', label='Train Loss', linewidth=2)
ax1.plot(history.history['val_loss'], 'b-', label='Val Loss', linewidth=2)
ax1.set_title('Model Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.set_ylim(0, max(history.history['val_loss'] + [0.1]) * 1.1)

# График точности
ax2.plot(history.history['accuracy'], 'g-', label='Train Acc', linewidth=2)
ax2.plot(history.history['val_accuracy'], 'b-', label='Val Acc', linewidth=2)
ax2.set_title('Model Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 9. ОЦЕНКА НА ТЕСТОВОЙ ВЫБОРКЕ
print("\n6. Оценка на тестовой выборке...")
test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose=0)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

# 10. ПОДРОБНЫЙ АНАЛИЗ ПРЕДСКАЗАНИЙ
print("\n7. Подробный анализ предсказаний...")
probs = model.predict(X_test, verbose=0)
pred_classes = np.argmax(probs, axis=1)

labels = ['Не спам', 'Спам']

# Confusion Matrix
cm = confusion_matrix(Y_test, pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Classification Report
print("\nClassification Report:")
print(classification_report(Y_test, pred_classes,
                          target_names=labels, digits=4))

# 11. ПРИМЕР ПРЕДСКАЗАНИЙ (первые 10)
print("\nПримеры предсказаний (первые 10):")
for i in range(min(10, len(X_test))):
    true_label = labels[Y_test[i]]
    pred_label = labels[pred_classes[i]]
    confidence = probs[i][pred_classes[i]]
    status = "✅" if pred_classes[i] == Y_test[i] else "❌"

    print(f"{status} Real: {true_label:<6} | Pred: {pred_label:<6} "
          f"(conf: {confidence:.3f})")

# 12. СОХРАНЕНИЕ МОДЕЛИ И SCALER'а
print("\n8. Сохранение модели...")
model.save('spam_classifier_model.h5')
joblib.dump(scaler, 'scaler.pkl')
print("✅ Модель и scaler сохранены!")

print("\n" + "="*60)
print("ОБУЧЕНИЕ ЗАВЕРШЕНО УСПЕШНО!")
print("="*60)

TensorFlow version: 2.19.0
============================================================
1. Загрузка данных...
Размер датасета: 20000 образцов, 5 признаков

2. Разделение данных...

3. Нормализация...
Обучающая выборка: 12000 образцов
Валидационная выборка: 4000 образцов
Тестовая выборка: 4000 образцов

4. Создание модели...
/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
Model: "sequential_11"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense_33 (Dense)                │ (None, 12)             │            72 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ activation_33 (Activation)      │ (None, 12)             │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_22 (Dropout)            │ (None, 12)             │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_34 (Dense)                │ (None, 6)              │            78 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ activation_34 (Activation)      │ (None, 6)              │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_23 (Dropout)            │ (None, 6)              │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_35 (Dense)                │ (None, 2)              │            14 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ activation_35 (Activation)      │ (None, 2)              │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 164 (656.00 B)
 Trainable params: 164 (656.00 B)
 Non-trainable params: 0 (0.00 B)

5. Обучение модели...
Epoch 1/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 4s 9ms/step - accuracy: 0.3955 - loss: 1.3287 - val_accuracy: 0.6180 - val_loss: 0.7784 - learning_rate: 5.0000e-04
Epoch 2/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5783 - loss: 0.8813 - val_accuracy: 0.7678 - val_loss: 0.5527 - learning_rate: 5.0000e-04
Epoch 3/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7152 - loss: 0.6702 - val_accuracy: 0.8430 - val_loss: 0.4374 - learning_rate: 5.0000e-04
Epoch 4/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7918 - loss: 0.5410 - val_accuracy: 0.8878 - val_loss: 0.3663 - learning_rate: 5.0000e-04
Epoch 5/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8409 - loss: 0.4688 - val_accuracy: 0.9130 - val_loss: 0.3188 - learning_rate: 5.0000e-04
Epoch 6/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8735 - loss: 0.4263 - val_accuracy: 0.9208 - val_loss: 0.2859 - learning_rate: 5.0000e-04
Epoch 7/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8925 - loss: 0.3836 - val_accuracy: 0.9268 - val_loss: 0.2618 - learning_rate: 5.0000e-04
Epoch 8/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8997 - loss: 0.3509 - val_accuracy: 0.9277 - val_loss: 0.2414 - learning_rate: 5.0000e-04
Epoch 9/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9116 - loss: 0.3093 - val_accuracy: 0.9277 - val_loss: 0.2270 - learning_rate: 5.0000e-04
Epoch 10/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9208 - loss: 0.2861 - val_accuracy: 0.9285 - val_loss: 0.2180 - learning_rate: 5.0000e-04
Epoch 11/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9227 - loss: 0.2771 - val_accuracy: 0.9298 - val_loss: 0.2120 - learning_rate: 5.0000e-04
Epoch 12/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9275 - loss: 0.2632 - val_accuracy: 0.9308 - val_loss: 0.2076 - learning_rate: 5.0000e-04
Epoch 13/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9249 - loss: 0.2544 - val_accuracy: 0.9310 - val_loss: 0.2037 - learning_rate: 5.0000e-04
Epoch 14/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9250 - loss: 0.2527 - val_accuracy: 0.9312 - val_loss: 0.2018 - learning_rate: 5.0000e-04
Epoch 15/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9285 - loss: 0.2415 - val_accuracy: 0.9315 - val_loss: 0.1993 - learning_rate: 5.0000e-04
Epoch 16/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9283 - loss: 0.2340 - val_accuracy: 0.9317 - val_loss: 0.1980 - learning_rate: 5.0000e-04
Epoch 17/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9330 - loss: 0.2341 - val_accuracy: 0.9317 - val_loss: 0.1961 - learning_rate: 5.0000e-04
Epoch 18/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9283 - loss: 0.2215 - val_accuracy: 0.9337 - val_loss: 0.1943 - learning_rate: 5.0000e-04
Epoch 19/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9310 - loss: 0.2250 - val_accuracy: 0.9342 - val_loss: 0.1930 - learning_rate: 5.0000e-04
Epoch 20/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9358 - loss: 0.2103 - val_accuracy: 0.9352 - val_loss: 0.1917 - learning_rate: 5.0000e-04
Epoch 21/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9335 - loss: 0.2152 - val_accuracy: 0.9355 - val_loss: 0.1908 - learning_rate: 5.0000e-04
Epoch 22/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9320 - loss: 0.2180 - val_accuracy: 0.9355 - val_loss: 0.1896 - learning_rate: 5.0000e-04
Epoch 23/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9358 - loss: 0.2166 - val_accuracy: 0.9367 - val_loss: 0.1883 - learning_rate: 5.0000e-04
Epoch 24/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9312 - loss: 0.2110 - val_accuracy: 0.9367 - val_loss: 0.1875 - learning_rate: 5.0000e-04
Epoch 25/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9298 - loss: 0.2159 - val_accuracy: 0.9375 - val_loss: 0.1864 - learning_rate: 5.0000e-04
Epoch 26/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9371 - loss: 0.2047 - val_accuracy: 0.9377 - val_loss: 0.1855 - learning_rate: 5.0000e-04
Epoch 27/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9308 - loss: 0.2108 - val_accuracy: 0.9385 - val_loss: 0.1843 - learning_rate: 5.0000e-04
Epoch 28/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9315 - loss: 0.2056 - val_accuracy: 0.9388 - val_loss: 0.1832 - learning_rate: 5.0000e-04
Epoch 29/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9357 - loss: 0.2016 - val_accuracy: 0.9390 - val_loss: 0.1822 - learning_rate: 5.0000e-04
Epoch 30/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.9360 - loss: 0.2003 - val_accuracy: 0.9390 - val_loss: 0.1816 - learning_rate: 5.0000e-04
Epoch 31/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9298 - loss: 0.2029 - val_accuracy: 0.9395 - val_loss: 0.1806 - learning_rate: 5.0000e-04
Epoch 32/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9372 - loss: 0.1984 - val_accuracy: 0.9395 - val_loss: 0.1801 - learning_rate: 5.0000e-04
Epoch 33/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9356 - loss: 0.1943 - val_accuracy: 0.9408 - val_loss: 0.1794 - learning_rate: 5.0000e-04
Epoch 34/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9349 - loss: 0.1939 - val_accuracy: 0.9415 - val_loss: 0.1784 - learning_rate: 5.0000e-04
Epoch 35/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9364 - loss: 0.1973 - val_accuracy: 0.9415 - val_loss: 0.1773 - learning_rate: 5.0000e-04
Epoch 36/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9359 - loss: 0.1979 - val_accuracy: 0.9405 - val_loss: 0.1769 - learning_rate: 5.0000e-04
Epoch 37/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9351 - loss: 0.1907 - val_accuracy: 0.9413 - val_loss: 0.1760 - learning_rate: 5.0000e-04
Epoch 38/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9384 - loss: 0.1868 - val_accuracy: 0.9415 - val_loss: 0.1751 - learning_rate: 5.0000e-04
Epoch 39/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9370 - loss: 0.1882 - val_accuracy: 0.9420 - val_loss: 0.1742 - learning_rate: 5.0000e-04
Epoch 40/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9354 - loss: 0.1915 - val_accuracy: 0.9423 - val_loss: 0.1738 - learning_rate: 5.0000e-04
Epoch 41/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9367 - loss: 0.1873 - val_accuracy: 0.9423 - val_loss: 0.1730 - learning_rate: 5.0000e-04
Epoch 42/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9400 - loss: 0.1853 - val_accuracy: 0.9435 - val_loss: 0.1721 - learning_rate: 5.0000e-04
Epoch 43/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9398 - loss: 0.1849 - val_accuracy: 0.9445 - val_loss: 0.1711 - learning_rate: 5.0000e-04
Epoch 44/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9401 - loss: 0.1801 - val_accuracy: 0.9442 - val_loss: 0.1706 - learning_rate: 5.0000e-04
Epoch 45/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9387 - loss: 0.1845 - val_accuracy: 0.9452 - val_loss: 0.1697 - learning_rate: 5.0000e-04
Epoch 46/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9398 - loss: 0.1780 - val_accuracy: 0.9460 - val_loss: 0.1692 - learning_rate: 5.0000e-04
Epoch 47/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9425 - loss: 0.1845 - val_accuracy: 0.9455 - val_loss: 0.1689 - learning_rate: 5.0000e-04
Epoch 48/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9399 - loss: 0.1821 - val_accuracy: 0.9463 - val_loss: 0.1681 - learning_rate: 5.0000e-04
Epoch 49/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9445 - loss: 0.1772 - val_accuracy: 0.9475 - val_loss: 0.1675 - learning_rate: 5.0000e-04
Epoch 50/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9410 - loss: 0.1821 - val_accuracy: 0.9467 - val_loss: 0.1667 - learning_rate: 5.0000e-04
Epoch 51/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9413 - loss: 0.1788 - val_accuracy: 0.9470 - val_loss: 0.1664 - learning_rate: 5.0000e-04
Epoch 52/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9401 - loss: 0.1756 - val_accuracy: 0.9475 - val_loss: 0.1657 - learning_rate: 5.0000e-04
Epoch 53/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9385 - loss: 0.1782 - val_accuracy: 0.9475 - val_loss: 0.1653 - learning_rate: 5.0000e-04
Epoch 54/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9408 - loss: 0.1778 - val_accuracy: 0.9475 - val_loss: 0.1652 - learning_rate: 5.0000e-04
Epoch 55/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9420 - loss: 0.1770 - val_accuracy: 0.9482 - val_loss: 0.1646 - learning_rate: 5.0000e-04
Epoch 56/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9424 - loss: 0.1744 - val_accuracy: 0.9473 - val_loss: 0.1642 - learning_rate: 5.0000e-04
Epoch 57/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9446 - loss: 0.1734 - val_accuracy: 0.9477 - val_loss: 0.1636 - learning_rate: 5.0000e-04
Epoch 58/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9425 - loss: 0.1735 - val_accuracy: 0.9492 - val_loss: 0.1636 - learning_rate: 5.0000e-04
Epoch 59/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9402 - loss: 0.1751 - val_accuracy: 0.9490 - val_loss: 0.1630 - learning_rate: 5.0000e-04
Epoch 60/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9443 - loss: 0.1698 - val_accuracy: 0.9480 - val_loss: 0.1624 - learning_rate: 5.0000e-04
Epoch 61/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9424 - loss: 0.1752 - val_accuracy: 0.9488 - val_loss: 0.1617 - learning_rate: 5.0000e-04
Epoch 62/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9418 - loss: 0.1707 - val_accuracy: 0.9475 - val_loss: 0.1612 - learning_rate: 5.0000e-04
Epoch 63/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9456 - loss: 0.1665 - val_accuracy: 0.9475 - val_loss: 0.1609 - learning_rate: 5.0000e-04
Epoch 64/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9437 - loss: 0.1692 - val_accuracy: 0.9488 - val_loss: 0.1602 - learning_rate: 5.0000e-04
Epoch 65/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9422 - loss: 0.1709 - val_accuracy: 0.9485 - val_loss: 0.1599 - learning_rate: 5.0000e-04
Epoch 66/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9450 - loss: 0.1688 - val_accuracy: 0.9490 - val_loss: 0.1591 - learning_rate: 5.0000e-04
Epoch 67/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9439 - loss: 0.1694 - val_accuracy: 0.9488 - val_loss: 0.1591 - learning_rate: 5.0000e-04
Epoch 68/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9432 - loss: 0.1719 - val_accuracy: 0.9498 - val_loss: 0.1584 - learning_rate: 5.0000e-04
Epoch 69/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9420 - loss: 0.1667 - val_accuracy: 0.9515 - val_loss: 0.1579 - learning_rate: 5.0000e-04
Epoch 70/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9434 - loss: 0.1661 - val_accuracy: 0.9513 - val_loss: 0.1574 - learning_rate: 5.0000e-04
Epoch 71/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9439 - loss: 0.1664 - val_accuracy: 0.9517 - val_loss: 0.1572 - learning_rate: 5.0000e-04
Epoch 72/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9450 - loss: 0.1639 - val_accuracy: 0.9503 - val_loss: 0.1566 - learning_rate: 5.0000e-04
Epoch 73/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9446 - loss: 0.1671 - val_accuracy: 0.9528 - val_loss: 0.1558 - learning_rate: 5.0000e-04
Epoch 74/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9467 - loss: 0.1617 - val_accuracy: 0.9515 - val_loss: 0.1559 - learning_rate: 5.0000e-04
Epoch 75/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9462 - loss: 0.1652 - val_accuracy: 0.9517 - val_loss: 0.1553 - learning_rate: 5.0000e-04
Epoch 76/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9453 - loss: 0.1634 - val_accuracy: 0.9515 - val_loss: 0.1551 - learning_rate: 5.0000e-04
Epoch 77/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9445 - loss: 0.1601 - val_accuracy: 0.9520 - val_loss: 0.1548 - learning_rate: 5.0000e-04
Epoch 78/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9457 - loss: 0.1632 - val_accuracy: 0.9532 - val_loss: 0.1543 - learning_rate: 5.0000e-04
Epoch 79/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9439 - loss: 0.1613 - val_accuracy: 0.9532 - val_loss: 0.1540 - learning_rate: 5.0000e-04
Epoch 80/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9448 - loss: 0.1606 - val_accuracy: 0.9530 - val_loss: 0.1535 - learning_rate: 5.0000e-04
Epoch 81/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9450 - loss: 0.1637 - val_accuracy: 0.9525 - val_loss: 0.1531 - learning_rate: 5.0000e-04
Epoch 82/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9433 - loss: 0.1632 - val_accuracy: 0.9530 - val_loss: 0.1531 - learning_rate: 5.0000e-04
Epoch 83/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9478 - loss: 0.1582 - val_accuracy: 0.9528 - val_loss: 0.1526 - learning_rate: 5.0000e-04
Epoch 84/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9471 - loss: 0.1586 - val_accuracy: 0.9530 - val_loss: 0.1525 - learning_rate: 5.0000e-04
Epoch 85/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9459 - loss: 0.1582 - val_accuracy: 0.9528 - val_loss: 0.1521 - learning_rate: 5.0000e-04
Epoch 86/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9473 - loss: 0.1559 - val_accuracy: 0.9530 - val_loss: 0.1519 - learning_rate: 5.0000e-04
Epoch 87/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9490 - loss: 0.1559 - val_accuracy: 0.9525 - val_loss: 0.1515 - learning_rate: 5.0000e-04
Epoch 88/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9460 - loss: 0.1604 - val_accuracy: 0.9528 - val_loss: 0.1514 - learning_rate: 5.0000e-04
Epoch 89/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9457 - loss: 0.1593 - val_accuracy: 0.9530 - val_loss: 0.1511 - learning_rate: 5.0000e-04
Epoch 90/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9489 - loss: 0.1555 - val_accuracy: 0.9525 - val_loss: 0.1508 - learning_rate: 5.0000e-04
Epoch 91/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9457 - loss: 0.1564 - val_accuracy: 0.9520 - val_loss: 0.1505 - learning_rate: 5.0000e-04
Epoch 92/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9454 - loss: 0.1554 - val_accuracy: 0.9520 - val_loss: 0.1503 - learning_rate: 5.0000e-04
Epoch 93/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9488 - loss: 0.1552 - val_accuracy: 0.9530 - val_loss: 0.1502 - learning_rate: 5.0000e-04
Epoch 94/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9468 - loss: 0.1576 - val_accuracy: 0.9520 - val_loss: 0.1500 - learning_rate: 5.0000e-04
Epoch 95/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9478 - loss: 0.1520 - val_accuracy: 0.9517 - val_loss: 0.1499 - learning_rate: 5.0000e-04
Epoch 96/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9466 - loss: 0.1538 - val_accuracy: 0.9525 - val_loss: 0.1495 - learning_rate: 5.0000e-04
Epoch 97/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9462 - loss: 0.1530 - val_accuracy: 0.9515 - val_loss: 0.1495 - learning_rate: 5.0000e-04
Epoch 98/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9484 - loss: 0.1601 - val_accuracy: 0.9515 - val_loss: 0.1491 - learning_rate: 5.0000e-04
Epoch 99/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9475 - loss: 0.1518 - val_accuracy: 0.9510 - val_loss: 0.1492 - learning_rate: 5.0000e-04
Epoch 100/100
94/94 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9477 - loss: 0.1547 - val_accuracy: 0.9515 - val_loss: 0.1489 - learning_rate: 5.0000e-04


6. Оценка на тестовой выборке...
Test Loss: 0.1363
Test Accuracy: 0.9532 (95.32%)

7. Подробный анализ предсказаний...

WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 

Classification Report:
              precision    recall  f1-score   support

     Не спам     0.9692    0.9796    0.9744      3633
        Спам     0.7744    0.6921    0.7309       367

    accuracy                         0.9533      4000
   macro avg     0.8718    0.8359    0.8527      4000
weighted avg     0.9514    0.9533    0.9521      4000


Примеры предсказаний (первые 10):
✅ Real: Не спам | Pred: Не спам (conf: 0.992)
✅ Real: Не спам | Pred: Не спам (conf: 1.000)
✅ Real: Спам   | Pred: Спам   (conf: 0.520)
✅ Real: Не спам | Pred: Не спам (conf: 1.000)
✅ Real: Не спам | Pred: Не спам (conf: 0.917)
✅ Real: Не спам | Pred: Не спам (conf: 0.847)
✅ Real: Не спам | Pred: Не спам (conf: 0.997)
✅ Real: Не спам | Pred: Не спам (conf: 0.974)
✅ Real: Не спам | Pred: Не спам (conf: 0.953)
✅ Real: Не спам | Pred: Не спам (conf: 0.999)

8. Сохранение модели...
✅ Модель и scaler сохранены!

============================================================
ОБУЧЕНИЕ ЗАВЕРШЕНО УСПЕШНО!
============================================================


Ссылка на Google Colab: https://colab.research.google.com/drive/1HJ71Vpt_X0uQ9sVNnHQDlkSHrZO6D_8G?usp=sharing
Ссылка на HTML-окно: https://disk.yandex.ru/d/Bd7VxpKTlvapHg
